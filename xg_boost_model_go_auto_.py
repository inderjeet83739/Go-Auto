# -*- coding: utf-8 -*-
"""XG Boost model Go Auto .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uyeD4AvUfhrQoBtjvr6lUfb0mWvPvwZL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('cleaneddataset.csv')

df

df['mileage_category'] = pd.qcut(df['mileage'], q=3, labels=['Low', 'Medium', 'High'])

df_new = df[df['stock_type'] == 'NEW']
df_used = df[df['stock_type'] == 'USED']

df_new['mileage']

df_used['mileage']

df_used['mileage_category']

df_new['mileage_category']

df_new['mileage_category'].value_counts()

df_used['mileage_category'].value_counts()

from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X = df[['mileage']]  # Features (mileage or other numerical features)
y = df['mileage_category']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split data
clf = LinearSVC(random_state=42, max_iter=10000) # Create a LinearSVC classifier
clf.fit(X_train, y_train)  # Train the classifier

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

df_new.to_csv('new_cleaned.csv')
df_used.to_csv('used_cleaned.csv')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.metrics import cohen_kappa_score

# Step 1: Load Cleaned Dataset
df_cleaned = pd.read_csv("used_cleaned.csv")  # Load the dataset

# Step 2: Create Mileage Groups (Low, Medium, High)
df_cleaned['mileage_group'] = pd.qcut(df_cleaned['mileage'], q=3, labels=['Low', 'Medium', 'High'])

# Step 3: Select Relevant Features
features = ['price', 'model_year', 'make', 'model', 'series', 'style',
            'wheelbase_from_vin', 'drivetrain_from_vin', 'engine_from_vin',
            'transmission_from_vin', 'fuel_type_from_vin', 'mileage']
target = 'mileage_group'

df_selected = df_cleaned[features + [target]].dropna()  # Drop missing values

# Step 4: Split Data into Train & Test Sets
X = df_selected.drop(columns=[target])
y = df_selected[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Step 5: Encode Categorical Features using OrdinalEncoder
categorical_cols = ['make', 'model', 'series', 'style', 'drivetrain_from_vin', 'engine_from_vin', 'transmission_from_vin', 'fuel_type_from_vin']

encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)  # Handle unseen labels

X_train[categorical_cols] = encoder.fit_transform(X_train[categorical_cols].astype(str))
X_test[categorical_cols] = encoder.transform(X_test[categorical_cols].astype(str))  # Avoid unseen-label errors

# Save encoder for later use
with open("ordinal_encoder.pkl", "wb") as f:
    pickle.dump(encoder, f)

correct_mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_train = y_train.map(correct_mapping)
y_test = y_test.map(correct_mapping)

# Print mapping manually (no need for LabelEncoder)
print(correct_mapping)

# Step 6: Train XGBoost Model
xgb_model = XGBClassifier(
    n_estimators=15,
    max_depth=1,
    learning_rate=0.002,
    subsample=0.8,
    colsample_bytree=0.4,
    reg_lambda=75.0,
    reg_alpha=50.0,
    random_state=42
)

xgb_model.fit(X_train, y_train)

# Step 7: Evaluate the Model
train_preds = xgb_model.predict(X_train)
test_preds = xgb_model.predict(X_test)

train_accuracy = accuracy_score(y_train, train_preds)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print("\nClassification Report:\n", classification_report(y_test, test_preds))

# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test, test_preds)
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Used Vehicles")
plt.show()

# Log-likelihood for the model
ll_model = np.sum(np.log(test_preds + 1e-9))  # Add small value to avoid log(0)

# Ensure y_test is numeric
y_test_numeric = y_test.astype(int)
test_preds_numeric = test_preds.astype(int)

# Calculate Cohen's Kappa Score
kappa_score = cohen_kappa_score(y_test_numeric, test_preds_numeric)
print(f"Cohen's Kappa Score: {kappa_score:.4f}")

# Get probability predictions instead of class labels
test_probs = xgb_model.predict_proba(X_test)  # This gives probability for each class

# Compute Log-Likelihood for the model
ll_model = np.sum(np.log(test_probs[np.arange(len(y_test)), y_test] + 1e-9))  # Select correct class probabilities

# Compute Log-Likelihood for the null model (always predicting the most frequent class)
p_null = np.bincount(y_test).argmax()  # Majority class
null_probs = np.full_like(y_test, p_null, dtype=np.float64)  # Null probabilities

ll_null = np.sum(np.log(null_probs + 1e-9))

# Compute McFadden's Pseudo R²
r_squared = 1 - (ll_model / ll_null)
print(f"McFadden's Pseudo R²: {r_squared:.4f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.metrics import cohen_kappa_score

# Step 1: Load Cleaned Dataset
df_cleaned = pd.read_csv("new_cleaned.csv")  # Load the dataset

# Step 2: Create Mileage Groups (Low, Medium, High)
df_cleaned['mileage_group'] = pd.qcut(df_cleaned['mileage'], q=3, labels=['Low', 'Medium', 'High'])

# Step 3: Select Relevant Features
features = ['price', 'model_year', 'make', 'model', 'series', 'style',
            'wheelbase_from_vin', 'drivetrain_from_vin', 'engine_from_vin',
            'transmission_from_vin', 'fuel_type_from_vin', 'mileage']
target = 'mileage_group'

df_selected = df_cleaned[features + [target]].dropna()  # Drop missing values

# Step 4: Split Data into Train & Test Sets
X = df_selected.drop(columns=[target])
y = df_selected[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Step 5: Encode Categorical Features using OrdinalEncoder
categorical_cols = ['make', 'model', 'series', 'style', 'drivetrain_from_vin', 'engine_from_vin', 'transmission_from_vin', 'fuel_type_from_vin']

encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)  # Handle unseen labels

X_train[categorical_cols] = encoder.fit_transform(X_train[categorical_cols].astype(str))
X_test[categorical_cols] = encoder.transform(X_test[categorical_cols].astype(str))  # Avoid unseen-label errors

# Save encoder for later use
with open("ordinal_encoder.pkl", "wb") as f:
    pickle.dump(encoder, f)

correct_mapping = {'Low': 0, 'Medium': 1, 'High': 2}
y_train = y_train.map(correct_mapping)
y_test = y_test.map(correct_mapping)

# Print mapping manually (no need for LabelEncoder)
print(correct_mapping)

# Step 6: Train XGBoost Model
xgb_model = XGBClassifier(
    n_estimators=100,
    max_depth=1,
    learning_rate=0.002,
    subsample=0.8,
    colsample_bytree=0.4,
    reg_lambda=75.0,
    reg_alpha=50.0,
    random_state=42
)

xgb_model.fit(X_train, y_train)

# Step 7: Evaluate the Model
train_preds = xgb_model.predict(X_train)
test_preds = xgb_model.predict(X_test)

train_accuracy = accuracy_score(y_train, train_preds)
test_accuracy = accuracy_score(y_test, test_preds)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")
print("\nClassification Report:\n", classification_report(y_test, test_preds))

# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test, test_preds)
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Medium', 'High'], yticklabels=['Low', 'Medium', 'High'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - New Vehicles")
plt.show()

# Log-likelihood for the model
ll_model = np.sum(np.log(test_preds + 1e-9))  # Add small value to avoid log(0)

# Ensure y_test is numeric
y_test_numeric = y_test.astype(int)
test_preds_numeric = test_preds.astype(int)

# Calculate Cohen's Kappa Score
kappa_score = cohen_kappa_score(y_test_numeric, test_preds_numeric)
print(f"Cohen's Kappa Score: {kappa_score:.4f}")

# Get probability predictions instead of class labels
test_probs = xgb_model.predict_proba(X_test)  # This gives probability for each class

# Compute Log-Likelihood for the model
ll_model = np.sum(np.log(test_probs[np.arange(len(y_test)), y_test] + 1e-9))  # Select correct class probabilities

# Compute Log-Likelihood for the null model (always predicting the most frequent class)
p_null = np.bincount(y_test).argmax()  # Majority class
null_probs = np.full_like(y_test, p_null, dtype=np.float64)  # Null probabilities

ll_null = np.sum(np.log(null_probs + 1e-9))

# Compute McFadden's Pseudo R²
r_squared = 1 - (ll_model / ll_null)
print(f"McFadden's Pseudo R²: {r_squared:.4f}")

